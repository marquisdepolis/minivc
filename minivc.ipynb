{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:56:08.783 Python[68150:3101895] +[CATransaction synchronize] called within transaction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of all text chunks is: \n",
      "1\n",
      "Team:\n",
      "Information not available.\n",
      "\n",
      "Customers:\n",
      "Anodot mainly focuses on the telecommunications sector, catering to businesses that seek to safeguard their revenue by identifying irregularities in service quality and accessibility. The solution aims to enhance service standards, reduce overhead expenses, boost profitability, and guarantee customer retention and network efficiency.\n",
      "\n",
      "Product:\n",
      "Anodot provides a Realtime Business Incident Detection solution specifically designed for telecom companies to safeguard their revenue by identifying service quality and availability anomalies. The product utilizes automatic anomaly detection, real-time AI analytics, and a patented machine learning technique. It examines large amounts of network data from various sources, optimizes service levels, reduces overhead expenses, increases profitability, and guarantees customer loyalty and network performance. Anodot can analyze terabytes of isolated network data, monitor and correlate numerous business KPIs from different data sources, address roaming problems, detect revenue leakage, and obtain real-time insights from customer usage patterns and behavior. The solution is built to scale, managing millions of metrics and analyzing complex data such as network usage, data traffic volume, and text and multimedia messages. Benefits of using Anodot include quicker insights, enhanced customer experience and dependable services, improved customer relationships and loyalty, proactive identification of trends and issues, a brief integration process, and immediate value and new efficiencies.\n",
      "\n",
      "Market:\n",
      "Information is currently unavailable.\n",
      "\n",
      "Business Model:\n",
      "Information not accessible\n",
      "\n",
      "Risks:\n",
      "Potential challenges for Anodot include competition from alternative AI-based anomaly detection systems, possible regulatory shifts in the telecommunications sector, and the development of cyber threats and security issues that may influence the efficacy of their real-time business incident detection.\n",
      "\n",
      "Traction:\n",
      "Information is currently unavailable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import openai\n",
    "import PyPDF2\n",
    "import requests\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pptx import Presentation\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tkinter import Tk, filedialog\n",
    "import readppt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "MODEL = \"gpt-4\"\n",
    "CHUNK_SIZE=7250\n",
    "ALLOWED_TLDS = {\"com\",\"app\", \"org\", \"net\", \"edu\", \"gov\"}\n",
    "EXCLUDED_KEYWORDS = {\n",
    "    \"login\",\n",
    "    \"signup\",\n",
    "    \"results\",\n",
    "    \"search\",\n",
    "    \"register\",\n",
    "    \"account\",\n",
    "    \"privacy\",\n",
    "    \"terms\",\n",
    "    \"policy\",\n",
    "    \"disclaimer\",\n",
    "    \"jobs\",\n",
    "    \"careers\",\n",
    "    \"blog\"\n",
    "    \"contact\",\n",
    "    \"cookie\",\n",
    "    \"support\",\n",
    "    \"forum\",\n",
    "    \"cdn\",\n",
    "    \"newsletter\",\n",
    "    \"status\",\n",
    "}\n",
    "FILENAME = \"analyzed_data.json\"\n",
    "\n",
    "def get_links(soup, base_url):\n",
    "    links = set()\n",
    "\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    ext = tldextract.extract(parsed_base_url.netloc)\n",
    "    base_domain = f\"{ext.domain}.{ext.suffix}\"\n",
    "    allowed_domains = {base_domain}\n",
    "\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        href = a_tag[\"href\"]\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = urljoin(base_url, href)\n",
    "        \n",
    "        parsed_url = urlparse(href)\n",
    "        ext = tldextract.extract(parsed_url.netloc)\n",
    "        domain = f\"{ext.domain}.{ext.suffix}\"\n",
    "\n",
    "        if (base_url in href\n",
    "            and domain in allowed_domains\n",
    "            and ext.suffix in ALLOWED_TLDS\n",
    "            and not any(keyword in href.lower() for keyword in EXCLUDED_KEYWORDS)\n",
    "        ):\n",
    "            links.add(href)\n",
    "    print(links)\n",
    "    return links\n",
    "\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {url}\\n{str(e)}\")\n",
    "        return None\n",
    "    return soup\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = \" \".join(text.split())\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'<script.*?>.*?</script>', '', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'<style.*?>.*?</style>', '', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9.,!?/:;()%$@&\\s]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'(?i)(terms\\s*and\\s*conditions|privacy\\s*policy|copyright|blog|legal|careers|cdn*).{0,10}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def split_text(text, chunk_size=CHUNK_SIZE):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = nlp(sentence)\n",
    "        sentence_length = len(tokens)\n",
    "\n",
    "        if current_chunk_size + sentence_length > chunk_size:\n",
    "            # Create a new chunk if adding the sentence would exceed the chunk size\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_size = sentence_length\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_size += sentence_length\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=2, max=20), stop=stop_after_attempt(3), reraise=True)\n",
    "def base_gptcall(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0]['message']['content'].strip()\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=2, max=20), stop=stop_after_attempt(3), reraise=True)\n",
    "def call_gpt(prompt):\n",
    "    answers = []\n",
    "    if len(prompt)>CHUNK_SIZE:\n",
    "        textchunks = split_text(prompt)\n",
    "        for chunk in textchunks:\n",
    "            answer = []\n",
    "            # print(len(chunk))\n",
    "            # print(chunk)\n",
    "            answer = base_gptcall(chunk)\n",
    "            answers.append(answer)\n",
    "        return ' '.join(answers)\n",
    "    else:\n",
    "        return base_gptcall(prompt)\n",
    "\n",
    "def recursive_analyze(text):\n",
    "    categories = [\n",
    "        \"Team\",\n",
    "        \"Customers\",\n",
    "        \"Product\",\n",
    "        \"Market\",\n",
    "        \"Business Model\",\n",
    "        \"Risks\",\n",
    "        \"Traction\"\n",
    "    ]\n",
    "    category_explanation = [\n",
    "        \"\"\"The team section should include the names of the CEO, co-founders and other team members and background if available.\n",
    "        Example: The CEO is Patrick Collison (ex-CEO of Auctomatic) and CTO is John Collison (ex-CTO of Auctomatic) who lead Stripe. Among its advisors include Patrick McKenzie.\"\"\",\n",
    "        \"\"\"The customers section should concentrate on target customer segments, industries, specific companies, and notable partnerships, without repeating information about product features or benefits.\n",
    "        Example: Stripe serves businesses of all sizes across various industries, from startups like Instacart to tech giants like Amazon, providing seamless payment solutions.\"\"\",\n",
    "        \"\"\"The product section should describe the main product(s) or service(s), highlighting key features, benefits, use cases, and unique selling points, without discussing market size or competition.\n",
    "        Example: Stripe offers a suite of payment processing services, including Stripe Payments for online transactions, Stripe Billing for subscription management, and Stripe Connect for marketplace platforms.\"\"\",\n",
    "        \"\"\"The market section should assess the market size, growth potential, and any adjacent opportunities, without reiterating information about the product, customers, or competition.\n",
    "        Example: Stripe operates in the global digital payments market, valued at over $4 trillion, with significant growth opportunities as e-commerce and digital transactions continue to rise.\"\"\",\n",
    "        \"\"\"The business model section should explain the startup's revenue generation methods and pricing strategies, without focusing on product features or competition.\n",
    "        Example: Stripe employs a pay-as-you-go pricing model, charging a percentage of each transaction, and offers additional features through tiered pricing plans and custom enterprise solutions.\"\"\",\n",
    "        \"\"\"The risks section should identify potential internal and external risks that could impact the investment, avoiding repetition of product features, benefits, or market size.\n",
    "        Example: Stripe faces competition from companies like PayPal and Square, potential regulatory changes affecting the fintech industry, and evolving cyber threats and security concerns.\"\"\",\n",
    "        \"\"\"The traction section should cover growth rates, user engagement metrics, milestones, and future goals, without discussing the product, competition, or market size.\n",
    "        Example: Stripe has experienced rapid growth, with millions of businesses using its platform, raising over $1.6 billion in funding, and expanding its services to over 40 countries.\"\"\"\n",
    "    ]\n",
    "\n",
    "    category_explanation_map = dict(zip(categories, category_explanation))\n",
    "    text_chunks = clean_text(text)\n",
    "    text_chunks = split_text(text)\n",
    "    print(\"The total length of all text chunks is: \")\n",
    "    print(len(text_chunks))\n",
    "    # Use ThreadPoolExecutor to parallelize GPT calls\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk in text_chunks:\n",
    "            futures.append(executor.submit(call_gpt, f\"Extract all insights, names and facts from the following text as would be useful for an investment memo:\\n\\n{chunk}\"))\n",
    "        insights_lists = [future.result() for future in futures]\n",
    "    insights_data = defaultdict(list)\n",
    "    combined_insights = \"\\n\".join(insights_lists)\n",
    "    for category in categories:\n",
    "        explanation = category_explanation_map[category]\n",
    "        prompt = f\"Imagining you to be writing a VC investment memo, from the following text please extract information regarding the category '{category}'. An example is here: {explanation}. If no useful information is present, please reply with 'info not available':\\n\\n{combined_insights}\"\n",
    "        summary = call_gpt(prompt)\n",
    "        insights_data[category].append(summary)\n",
    "    return insights_data\n",
    "\n",
    "def link(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    if not parsed_url.scheme or not parsed_url.hostname:\n",
    "        print(\"Invalid URL. Please provide a valid URL with a scheme (e.g., http:// or https://).\")\n",
    "        return None\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.hostname\n",
    "    soup = fetch_html(url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    links = get_links(soup, base_url)\n",
    "    all_text = []\n",
    "    for link in links:\n",
    "        sub_soup = fetch_html(link)\n",
    "        if sub_soup:\n",
    "            text = clean_text(sub_soup.get_text())\n",
    "            all_text.append(text)\n",
    "    full_text = \" \".join(all_text)\n",
    "    full_text = clean_text(full_text)\n",
    "    # print(f\"Full text is: {full_text}\")\n",
    "    analyzed_data = recursive_analyze(full_text)\n",
    "    return analyzed_data\n",
    "\n",
    "def read_pdf(file):\n",
    "    file.seek(0)  # move the file cursor to the beginning\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    if len(pdf_reader.pages) == 0:\n",
    "        raise ValueError(\"PDF file is empty\")\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += pdf_reader.pages[page_num].extract_text()\n",
    "    cleaned_text = clean_text(text)\n",
    "    return cleaned_text\n",
    "\n",
    "def get_file_input(input_type):\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    if input_type == \"pdf\":\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    elif input_type == \"pptx\":\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"PowerPoint files\", \"*.pptx\")])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "def analyze_input(input_type, company, url):\n",
    "    text = \"\"\n",
    "    if input_type == \"url\":\n",
    "        data = link(url)\n",
    "    elif input_type in [\"pdf\", \"pptx\"]:\n",
    "        file_path = get_file_input(input_type)\n",
    "        if not file_path:\n",
    "            print(\"No file selected.\")\n",
    "            return\n",
    "\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            if input_type == \"pdf\":\n",
    "                text = read_pdf(file)\n",
    "            elif input_type == \"pptx\":\n",
    "                file_content = file.read()\n",
    "                text = readppt.read_ppt(file_content)\n",
    "        data = recursive_analyze(text)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type\")\n",
    "    \n",
    "    company_data = []\n",
    "    for category, summary in data.items():\n",
    "        edited_summary = call_gpt(f\"Please rewrite this summary:{summary}\")\n",
    "        print(f\"{category}:\\n{edited_summary}\\n\")\n",
    "        data_to_save = {\n",
    "            \"category\": category,\n",
    "            \"edited_summary\": edited_summary\n",
    "        }\n",
    "        company_data.append(data_to_save)    \n",
    "    save_data(FILENAME, company, company_data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def save_data(filename, company, data):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(filename, \"r\") as f:\n",
    "        all_data = json.load(f)\n",
    "    if company not in all_data:\n",
    "        all_data[company] = {}\n",
    "    all_data[company][timestamp] = data\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "\n",
    "def run():\n",
    "    input_type = input(\"Enter the input type (URL/PDF/PPTX): \").lower()\n",
    "    company = input(\"Enter company name: \")\n",
    "    if input_type == \"url\":\n",
    "        url = input(\"Enter a URL: \")\n",
    "    else:\n",
    "        url = None\n",
    "    analyze_input(input_type, company, url)\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
